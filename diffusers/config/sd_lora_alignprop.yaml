exp_name: debug

trainer:
  seed: 42
  target: diffusers.model.rl_trainers.TrainerAlignProp
  model_config:
    backprop_strategy: "gaussian"
    backprop_kwargs:
      gaussian:
        mean: 42
        std: 5
      uniform:
        min: 0
        max: 50
      fixed:
        value: 49
    unet_config:
      target: diffusers.model.stable_diffusion_stabilityai.UNetModel
      params:
        image_size: 32
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions: [4, 2, 1]
        num_res_blocks: 2
        channel_mult: [1, 2, 4, 4]
        num_heads: 8
        use_spatial_transformer: True
        transformer_depth: 1
        context_dim: 768
        use_checkpoint: False
        legacy: False
        pretrained: diffusers/pretrained/sd-v1-5-pruned-unet-ema.safetensors
    first_stage_config:
      target: diffusers.model.vae.AutoEncoderKL
      params:
        in_channels: 3
        base_channels: 128
        out_channels: 3
        z_channels: 4
        ch_mult: [1, 2, 4, 4]
        num_res_blocks: 2
        attn_resolutions: []
        dropout: 0.0
        scale_factor: 0.18215
        pretrained: diffusers/pretrained/sd-v1-5-pruned-vae.pth
    cond_stage_config:
      target: diffusers.model.text_encoder.CLIPTextEncoder
      params:
        version: openai/clip-vit-large-patch14
        layer: last
        layer_idx: -1
    lora:
      lora_rank: 4
      lora_alpha: 4
      dropout_module: 0.0
      dropout: 0.0
      dropout_rank: 0.0
    sampler_config:
      target: diffusers.sampler.sample_ddim.DDIMSampler
      params:
        num_train_timesteps: 1000
        num_inference_timesteps: 50
        beta_start: 0.00085
        beta_end: 0.012
        beta_schedule: sqrt_linear
        eta: 1.0
  loss_config:
    target: diffusers.loss.rewards.aesthetic_loss_fn
    loss_coeff: 0.01
    aesthetic_target: 10.0
  optimizer_config:
    optimizer: torch.optim.AdamW
    optimizer_params:
      lr: 0.001
      betas: [0.9, 0.999]
      weight_decay: 1e-4
    lr_scheduler: torch.optim.lr_scheduler.CosineAnnealingLR
    lr_scheduler_params:
      T_max: 1000000
      eta_min: 0.001

data:
  batch_size: 128
  val_batch_size: 8
  num_workers: 1
  train:
    target: diffusers.data.simple_datasets.SimpleAnimalPrompts
    params:
      data_dir:
      length: 2560    # means 20 sampling and training epochs * 128 samples per epoch
  val:
    - name: simple_animal
      target: diffusers.data.simple_datasets.SimpleAnimalPrompts
      params:
        data_dir:
        length: 10
  test:
    - name: simple_animal
      target: diffusers.data.simple_datasets.SimpleAnimalPrompts
      params:
        data_dir:
        length: 10

train_config:
  num_epochs: 1
  eval_interval: 100
  ckpt_interval: 10000
  log_interval: 1
  save_optimizer_states: False