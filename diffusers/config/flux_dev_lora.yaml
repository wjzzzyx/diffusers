exp_name: debug

trainer:
  seed: 42
  target: diffusers.model.flux_lora.Trainer
  model_config:
    lora:
      multiplier: 1.0
      lora_dim: 16
      lora_alpha: 16
      dropout_module: 0.0
      dropout: 0.0
      dropout_rank: 0.0
    flow:
      pretrained: 
      in_channels: 64
      vec_in_dim: 768
      context_in_dim: 4096
      hidden_size: 3072
      mlp_ratio: 4.0
      num_heads: 24
      depth: 19
      depth_single_blocks: 38
      axes_dim: [16, 56, 56]
      theta: 10000
      qkv_bias: True
      guidance_embed: True
    ae:
      pretrained:
      resolution: 256
      in_channels: 3
      base_channels: 128
      out_channels: 3
      ch_mult: [1, 2, 4, 4]
      num_res_blocks: 2
      z_channels: 16
      scale_factor: 0.3611
      shift_factor: 0.1159
    loss_config: null
    optimizer_config:
      optimizer: torch.optim.AdamW
      base_lr: 0.0001
      lr_scheduler: torch.optim.lr_scheduler.CosineAnnealingLR
      lr_scheduler_params:
        T_max: 100000
    shift_schedule: False

data:
  batch_size: 8
  val_batch_size: 8
  num_workers: 1
  train:
    target:
    params:
      root_dir:
      mode: train
  val:
    - name:
      target:
      params:
        root_dir:
        mode: val

train_config:
  num_epochs: 100
  eval_interval: 10
  ckpt_interval: 10
  log_interval: 10
  save_optimizer_states: True